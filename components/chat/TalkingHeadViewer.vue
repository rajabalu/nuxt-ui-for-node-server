<template>
  <div ref="viewerContainer" class="viewer-container"></div>
</template>

<script setup>
import { ref, onMounted, onUnmounted, defineExpose } from 'vue';
import { TalkingHead } from '~/libs/talkinghead.mjs';

const props = defineProps({
  modelUrl: {
    type: String,
    required: true,
    default: '/models/Leo.glb'
  }
});

const viewerContainer = ref(null);
let talkingHead = null;
let audioContext = null;
let audioSourceNode = null;
let gainNode = null;

// Debug flag - set to true to enable detailed logging
const DEBUG = true;

// --- Helper functions ---
const log = (message, ...args) => {
  if (DEBUG) {
    console.log(`[TalkingHeadViewer] ${message}`, ...args);
  }
};

// Viseme and word tracking for lipsync
const visemeMap = [
  /* 0  */ "sil",            // Silence
  /* 1  */ "aa",             // Ã¦, É™, ÊŒ
  /* 2  */ "aa",             // É‘
  /* 3  */ "O",              // É”
  /* 4  */ "E",              // É›, ÊŠ
  /* 5  */ "RR",             // É
  /* 6  */ "I",              // j, i, Éª
  /* 7  */ "U",              // w, u
  /* 8  */ "O",              // o
  /* 9  */ "O",              // aÊŠ
  /* 10 */ "O",              // É”Éª
  /* 11 */ "I",              // aÉª
  /* 12 */ "kk",             // h
  /* 13 */ "RR",             // É¹
  /* 14 */ "nn",             // l
  /* 15 */ "SS",             // s, z
  /* 16 */ "CH",             // Êƒ, tÊƒ, dÊ’, Ê’
  /* 17 */ "TH",             // Ã°
  /* 18 */ "FF",             // f, v
  /* 19 */ "DD",             // d, t, n, Î¸
  /* 20 */ "kk",             // k, g, Å‹
  /* 21 */ "PP"              // p, b, m
];

// --- Buffer management for lip-sync and audio ---
// Match buffers exactly as in the example file
let visemeBuffer = {
  visemes: [],
  vtimes: [],
  vdurations: []
};
let prevViseme = null;
let wordBuffer = {
  words: [],
  wtimes: [],
  wdurations: []
};
let lipsyncType = "visemes"; // Default to visemes mode

// Reset buffers between speech segments
const resetBuffers = () => {
  visemeBuffer = {
    visemes: [],
    vtimes: [],
    vdurations: []
  };
  prevViseme = null;
  wordBuffer = {
    words: [],
    wtimes: [],
    wdurations: []
  };
};

// Ensure audio context and worklet are properly initialized
const ensureAudioContextReady = async () => {
  if (!talkingHead) {
    console.error("TalkingHead not initialized");
    return false;
  }
  
  try {
    if (talkingHead.audioCtx && talkingHead.audioCtx.state === "suspended") {
      await talkingHead.audioCtx.resume();
    }
    return true;
  } catch (error) {
    console.error("Error initializing audio context:", error);
    return false;
  }
};

// --- Exposed Methods ---
// Method to ensure audio context is resumed - this needs to be called on user interaction
const resumeAudioContext = async () => {
  if (!talkingHead) return false;
  
  try {
    // Access and resume the audio context from the TalkingHead instance
    if (talkingHead.audioCtx && talkingHead.audioCtx.state === "suspended") {
      console.log("Resuming suspended audio context...");
      await talkingHead.audioCtx.resume();
      console.log("Audio context resumed successfully, state:", talkingHead.audioCtx.state);
      return true;
    }
    return true; // Already running
  } catch (error) {
    console.error("Error resuming audio context:", error);
    return false;
  }
};

// Method for handling viseme data (called by parent component)
const processViseme = (visemeId, audioOffset) => {
  if (!talkingHead || !talkingHead.isStreaming) return;
  
  const vtime = audioOffset / 10000; // Convert to milliseconds
  const viseme = visemeMap[visemeId];
  
  // Process viseme data exactly as in example
  if (prevViseme) {
    let vduration = vtime - prevViseme.vtime;
    if (vduration < 40) vduration = 40; // Minimum duration
    
    visemeBuffer.visemes.push(prevViseme.viseme);
    visemeBuffer.vtimes.push(prevViseme.vtime);
    visemeBuffer.vdurations.push(vduration);
  }
  
  prevViseme = { viseme, vtime };
};

// Process final viseme (called when speech synthesis is complete)
const processFinalViseme = () => {
  if (prevViseme) {
    // Add final viseme with estimated duration - same as example
    const finalDuration = 100;
    visemeBuffer.visemes.push(prevViseme.viseme);
    visemeBuffer.vtimes.push(prevViseme.vtime);
    visemeBuffer.vdurations.push(finalDuration);
    
    // Clear the last viseme
    prevViseme = null;
  }
  
  // Send any remaining data
  let finalData = {};
  
  // Mirror example file logic for final data
  if (visemeBuffer.visemes.length) {
    finalData.visemes = visemeBuffer.visemes.splice(0, visemeBuffer.visemes.length);
    finalData.vtimes = visemeBuffer.vtimes.splice(0, visemeBuffer.vtimes.length);
    finalData.vdurations = visemeBuffer.vdurations.splice(0, visemeBuffer.vdurations.length);
  }
  
  // Stream words always for subtitles
  finalData.words = wordBuffer.words.splice(0, wordBuffer.words.length);
  finalData.wtimes = wordBuffer.wtimes.splice(0, wordBuffer.wtimes.length);
  finalData.wdurations = wordBuffer.wdurations.splice(0, wordBuffer.wdurations.length);
  
  if (finalData.visemes || finalData.words) {
    // If we have any visemes or words left, stream them
    finalData.audio = null;
    talkingHead.streamAudio(finalData);
  }
  
  // Notify the end of streaming
  talkingHead.streamNotifyEnd();
  
  // Reset buffers
  resetBuffers();
};

// Method for handling word boundary data
const processWordBoundary = (text, audioOffset, duration, boundaryType) => {
  const time = audioOffset / 10000;
  const durationMs = duration / 10000;
  
  // Match exactly the example file logic
  if (boundaryType === "PunctuationBoundary" && wordBuffer.words.length) {
    // Append punctuation to previous word
    wordBuffer.words[wordBuffer.words.length - 1] += text;
    wordBuffer.wdurations[wordBuffer.wdurations.length - 1] += durationMs;
  } else if (boundaryType === "WordBoundary" || boundaryType === "PunctuationBoundary") {
    wordBuffer.words.push(text);
    wordBuffer.wtimes.push(time);
    wordBuffer.wdurations.push(durationMs);
  }
};

// Add variables to maintain accumulated audio data
const audioChunks = ref([]);
const isPlayingAccumulatedAudio = ref(false);

// Method to collect all audio chunks and play them as one continuous stream
const playAccumulatedAudioChunks = async () => {
  if (!audioChunks.value.length) {
    console.log("No audio chunks to play");
    return;
  }

  if (isPlayingAccumulatedAudio.value) {
    console.log("Already playing accumulated audio");
    return;
  }
  
  try {
    isPlayingAccumulatedAudio.value = true;
    
    // Create a new audio context for the continuous playback
    if (!window.__continuousAudioContext) {
      window.__continuousAudioContext = new (window.AudioContext || window.webkitAudioContext)();
    }
    
    if (window.__continuousAudioContext.state === "suspended") {
      console.log("ðŸ”ŠðŸ”Š Resuming continuous audio context...");
      await window.__continuousAudioContext.resume();
    }
    
    console.log(`ðŸ”ŠðŸ”Š Playing ${audioChunks.value.length} accumulated audio chunks...`);
    
    // Calculate total size
    const totalSize = audioChunks.value.reduce((total, chunk) => total + chunk.byteLength, 0);
    console.log(`ðŸ”ŠðŸ”Š Total audio size: ${totalSize} bytes`);
    
    // Combine all chunks into one buffer
    const combinedBuffer = new Uint8Array(totalSize);
    let offset = 0;
    
    audioChunks.value.forEach(chunk => {
      const chunkView = new Uint8Array(chunk);
      combinedBuffer.set(chunkView, offset);
      offset += chunk.byteLength;
    });
    
    // Decode and play the combined buffer
    window.__continuousAudioContext.decodeAudioData(
      combinedBuffer.buffer,
      (decodedBuffer) => {
        console.log(`ðŸ”ŠðŸ”Š Successfully decoded combined audio: ${decodedBuffer.duration} seconds`);
        
        const source = window.__continuousAudioContext.createBufferSource();
        source.buffer = decodedBuffer;
        
        // Add gain control
        const gainNode = window.__continuousAudioContext.createGain();
        gainNode.gain.value = 1.0; // Full volume
        
        // Connect the nodes
        source.connect(gainNode);
        gainNode.connect(window.__continuousAudioContext.destination);
        
        // Start playback
        console.log("ðŸ”ŠðŸ”Š Starting continuous audio playback");
        source.start(0);
        
        // Handle playback completion
        source.onended = () => {
          console.log("ðŸ”ŠðŸ”Š Continuous audio playback completed");
          isPlayingAccumulatedAudio.value = false;
          // Clear the chunks after playback
          audioChunks.value = [];
        };
      },
      (error) => {
        console.error("ðŸ”´ Error decoding combined audio:", error);
        isPlayingAccumulatedAudio.value = false;
      }
    );
  } catch (error) {
    console.error("ðŸ”´ Error playing accumulated audio:", error);
    isPlayingAccumulatedAudio.value = false;
  }
};

// Method to play audio chunks
const playAudioChunk = (audioData) => {
  // Remove debugger statement
  if (!talkingHead || !talkingHead.isStreaming) {
    console.warn("TalkingHead not streaming, cannot play audio");
    return;
  }
  
  // Basic diagnostics
  console.log(`Received audio chunk: ${audioData?.byteLength ?? 'undefined'} bytes`);
  
  try {
    if (!audioData || audioData.byteLength === 0) {
      console.error("Empty or null audio data received");
      return;
    }
    
    // IMPORTANT: We're no longer sending audio to TalkingHead directly
    // but still need to process visemes for lip syncing
    switch (lipsyncType) {
      case "blendshapes":
        // Skip sending audio but process animation data
        talkingHead.streamAudio({
          // Removed audio: audioData to prevent immediate playback
          anims: azureBlendShapes?.sbuffer.splice(0, azureBlendShapes?.sbuffer.length)
        });
        break;
      case "visemes":
        // Skip sending audio but process visemes for lip sync
        talkingHead.streamAudio({
          // Removed audio: audioData to prevent immediate playback
          visemes: visemeBuffer.visemes.splice(0, visemeBuffer.visemes.length),
          vtimes: visemeBuffer.vtimes.splice(0, visemeBuffer.vtimes.length),
          vdurations: visemeBuffer.vdurations.splice(0, visemeBuffer.vdurations.length),
        });
        break;
      case "words":
        // Skip sending audio but process words for lip sync
        talkingHead.streamAudio({
          // Removed audio: audioData to prevent immediate playback
          words: wordBuffer.words.splice(0, wordBuffer.words.length),
          wtimes: wordBuffer.wtimes.splice(0, wordBuffer.wtimes.length),
          wdurations: wordBuffer.wdurations.splice(0, wordBuffer.wdurations.length)
        });
        break;
      default:
        console.error(`Unknown animation mode: ${lipsyncType}`);
    }
    
    log("Animation data sent to TalkingHead (without audio)");
    
    // Still collect audio chunks for the accumulated playback at the end
    audioChunks.value.push(audioData);
  } catch (error) {
    console.error("Error processing animation data:", error);
  }
};

const reset = () => {
  console.log("Resetting TalkingHead state");
  if (talkingHead) {
    try {
      // Stop any ongoing speech
      if (talkingHead.isSpeaking) {
        talkingHead.streamStop();
      }
      resetBuffers();
    } catch (error) {
      console.error("Error resetting TalkingHead:", error);
    }
  }
};

// Start streaming mode to prepare for audio chunks
const startStreaming = async () => {
  if (!talkingHead) {
    console.error("TalkingHead not initialized");
    return false;
  }
  
  try {
    // Reset buffers for new speech
    resetBuffers();
    
    // Mirror example file exactly
    talkingHead.streamStart(
      { 
        sampleRate: 48000, // 48kHz sample rate for Azure Raw48Khz16BitMonoPcm
        mood: "neutral",
        gain: 0.5, 
        lipsyncType: lipsyncType 
      },
      // Start callback - called when audio playback starts
      () => {
        console.log("TalkingHead audio playback started");
        // Could add subtitles reset here if needed
      },
      // End callback - called when audio playback ends
      () => {
        console.log("TalkingHead audio playback ended");
        // Could handle subtitle cleanup here if needed
      },
      // Subtitle callback
      (subtitleText) => {
        console.log("Subtitle text:", subtitleText);
        // Could display subtitles here if needed
      }
    );
    
    return true;
  } catch (error) {
    console.error("Error starting TalkingHead streaming:", error);
    return false;
  }
};

// --- Lifecycle Hooks ---
onMounted(async () => {
  if (!viewerContainer.value) return;

  try {
    // Initialize TalkingHead with the container DOM element
    talkingHead = new TalkingHead(viewerContainer.value, {
      ttsEndpoint: 'placeholder', // Dummy values to satisfy constructor
      jwtGet: () => 'dummy-token',
      avatarIdleEyeContact: 0.5,
      avatarIdleHeadMove: 0.5,
      avatarSpeakingEyeContact: 0.8,
      avatarSpeakingHeadMove: 0.8,
      cameraView: "upper" // Upper body view like in the example
    });

    console.log('TalkingHead initialized.');
    
    // Load the model/avatar
    await talkingHead.showAvatar({ url: props.modelUrl });
    console.log('Avatar loaded successfully');
    
  } catch (error) {
    console.error('Error initializing TalkingHead:', error);
  }
});

onUnmounted(() => {
  if (talkingHead) {
    try {
      // Stop any ongoing speech
      if (talkingHead.isStreaming) {
        talkingHead.streamStop();
      }
      
      // Stop animation and clean up
      talkingHead.stop();
    } catch (error) {
      console.error("Error cleaning up TalkingHead:", error);
    }
  }
  
  resetBuffers();
  talkingHead = null;
});

// Expose methods for parent component to call
defineExpose({
  playAudioChunk,
  playAccumulatedAudioChunks,
  processViseme,
  processWordBoundary,
  processFinalViseme,
  startStreaming,
  reset,
  resumeAudioContext
});
</script>

<style scoped>
.viewer-container {
  width: 100%;
  height: 800px; /* Set a fixed height */
  min-height: 300px;
  position: relative; /* Needed for potential overlays */
  overflow: hidden; /* Ensure canvas doesn't overflow */
  background-image: url('../../public/images/background/DarkBackground.png'); /* Optional background image */
  background-size: cover; /* Cover the entire container */
  background-position: center; /* Center the background image */
}
</style>